---
title: "Getting Started with FinalProject13"
output: rmarkdown::html_vignette
author: "Braxton Stacey, Joshua Fahlgren, Thomas Jackson"
vignette: >
  %\VignetteIndexEntry{tutorial_name}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(FinalProject13)
```

## Introduction

Welcome to the **FinalProject13** package! This package implements supervised binary classification using numerical optimization techniques, with a particular focus on logistic regression. It is designed to help users efficiently estimate the coefficients of logistic regression models and generate confidence intervals using bootstrapping. Additionally, the package includes tools for model evaluation, including confusion matrix calculations and other diagnostic metrics.

Whether you are exploring relationships in your data or building robust predictive models, **FinalProject13** provides a set of functions for model fitting, evaluation, and interpretation, helping you easily integrate these tools into your analytical workflow.

In this vignette, we will guide you through the primary functionalities of the package, showing you how to:
- Estimate logistic regression coefficients using numerical optimization
- Generate bootstrap confidence intervals for model parameters
- Make predictions and evaluate model performance using various metrics

Let's get started!

## Data Preprocessing

Before we begin using the functions in the **FinalProject13** package, let's first preprocess the data. For this example, we will be using a dataset called **expenses.csv**, which contains various information, including smoking status, BMI, and the number of children for a set of individuals. Our goal is to process this data and prepare it for binary classification analysis.

### Step 1: Loading the Data

We will begin by loading the **expenses.csv** file. This dataset includes multiple columns, but we are particularly interested in three columns:
- The **smoker** column (5th column), which indicates whether an individual smokes or not (with values "yes" or "no").
- The **bmi** column (3rd column), which represents the Body Mass Index (BMI) of each individual.
- The **children** column (2nd column), which represents the number of children an individual has.

```{r}
# Load the dataset from the 'inst/extdata' directory in the package
expenses <- read.csv(system.file("extdata", "expenses.csv", package = "FinalProjectGroup13"))
head(expenses)

# Step 2: Converting Categorical Data to Binary
# Convert 'smoker' column to binary (1 for yes, 0 for no)
# Set the median value for charges
median_charges <- 9382

# Create a new column indicating whether charges are greater than the median
expenses$charges_above_median <- ifelse(expenses$charges > median_charges, 1, 0)


# Create a new column for age in years older than 18
expenses$years_older_than_min <- expenses$age - 18

# Create a new column for age in years older than 18
expenses$bmi_greater_than_min <- expenses$bmi - 15.96

# Step 3: Inspecting the Data
# Inspect the 'bmi' and 'children' columns for any issues
summary(expenses$years_older_than_min)
summary(expenses$bmi_greater_than_min)

# Step 4: Data Summary
# Summary of the processed data
summary(expenses)
```

## Applying Package Functions to the Data

Now that we have preprocessed the data, let's begin applying the functions from the **FinalProject13** package to perform logistic regression analysis on the data. We will use the **smoker** and **bmi** columns for this analysis.

### Step 1: Estimating Beta Coefficients Using Logistic Regression

The first step in logistic regression is to estimate the coefficients for the predictor variables. We will use the `estimate_beta()` function to perform this estimation. The function takes a matrix of predictor variables (in our case, the **bmi** column) and a binary response variable (the **smoker** column), and returns the estimated beta coefficients.
```{r}
# Ensure that both 'bmi' and 'children' columns are numeric
expenses$years_older_than_min <- as.numeric(expenses$years_older_than_min)
expenses$bmi_greater_than_min <- as.numeric(expenses$bmi_greater_than_min)

# Create the predictor matrix (bmi and children columns) as a numeric matrix
X <- as.matrix(expenses[, c("years_older_than_min", "bmi_greater_than_min")])

# Response vector (smoker column)
y <- expenses$charges_above_median

# Apply the estimate_beta function to estimate the beta coefficients
model <- estimate_beta(X, y)

# View the estimated coefficients
model$coefficients

```

#### Intercept (`β₀ = -1.4026`):
The intercept represents the log-odds of being a smoker when the BMI is zero. In this case, the log-odds of being a smoker is approximately -1.40, which corresponds to a probability of being a smoker less than 0.5 when the BMI is zero.

#### Coefficient for BMI (`β₁ = 0.0015`):
The coefficient for BMI represents the change in the log-odds of being a smoker for each additional unit increase in BMI. In this case, for each additional unit increase in BMI, the log-odds of being a smoker increase by approximately 0.0015. This suggests that the probability of being a smoker increases slightly with an increase in BMI, though the effect is very small as the coefficient is close to zero.


#### Explanation:

Here’s how the `estimate_beta()` function works:

1. **Adding the Intercept Term:**
   The first step is to add a column of ones to the predictor matrix \(X\) to account for the intercept term in the logistic regression model. This is necessary because the logistic regression model includes a constant term (\(\beta_0\)) that does not depend on the predictors.

2. **Calculating Starting Beta Coefficients:**
   Next, the function calculates the starting values for the beta coefficients using Ordinary Least Squares (OLS) regression. This is done by solving the normal equation:

   \[
   \beta_{\text{OLS}} = (X^T X)^{-1} X^T y
   \]

   where \(X\) is the predictor matrix (including the intercept term), and \(y\) is the response variable (whether the individual smokes or not, coded as 1 or 0). These initial estimates are used as starting values for the optimization algorithm.

3. **Defining the Loss Function:**
   The loss function used in logistic regression is the negative log-likelihood function. This function measures how well the model fits the data by comparing the predicted probabilities to the actual response values. The logistic (sigmoid) function is applied to the linear combination of the predictors and their corresponding coefficients, and the log-likelihood is computed.

   The formula for the log-likelihood for logistic regression is:

   \[
   \text{Log-Likelihood} = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
   \]

   where \(p_i\) is the predicted probability for the \(i\)-th observation, calculated using the logistic function.

4. **Optimization:**
   The function uses the `optim()` function in R to find the values of the beta coefficients that minimize the negative log-likelihood. The optimization process adjusts the beta coefficients to find the best-fitting model. The `optim()` function returns the optimized beta coefficients that minimize the loss function.

5. **Returning the Results:**
   Once the optimization is complete, the function returns a list containing:
   - The optimized beta coefficients (`coefficients`),
   - The fitted probabilities (`fitted_values`),
   - The starting beta coefficients from the OLS estimation (`starting_beta`), and
   - Information about the optimization process, including the value of the loss function and the number of iterations.

The `estimate_beta()` function provides the estimated coefficients for the logistic regression model, which can then be used for predictions, hypothesis testing, or model evaluation.


### Step 2: Predicting Probabilities for Logistic Regression

After estimating the beta coefficients using the logistic regression model, the next step is to predict the probabilities of the outcome variable (whether an individual is a smoker) for each observation in the dataset. This is done using the `predict_prob()` function from our package.

The `predict_prob()` function takes the predictor matrix \(X\) (which includes the **bmi** variable) and the estimated beta coefficients, and it returns the predicted probabilities for each observation in the dataset. These predicted probabilities indicate the likelihood that each individual is a smoker based on their BMI.

The formula used for prediction is the logistic function applied to the linear predictor:

\[
p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 \cdot \text{BMI})}}
\]

where:
- \(p_i\) is the predicted probability of being a smoker for the \(i\)-th observation,
- \(\beta_0\) is the intercept,
- \(\beta_1\) is the coefficient for BMI,
- and \(\text{BMI}\) is the predictor variable.

Using the estimated coefficients, the `predict_prob()` function computes the probability for each observation in the dataset, which can then be used for classification or further model evaluation.


```{r}
# Apply the predict_prob function to generate predicted probabilities
predictions <- predict_prob(X, model$coefficients)

# View the predicted probabilities for the first few observations
head(predictions)

```
#### Predicted Probabilities:

The predicted probabilities represent the likelihood of an individual being a smoker, based on their BMI. For the first few observations, the predicted probabilities range from 0.2041 to 0.2036. This means that for these observations, the model predicts a relatively low probability of being a smoker.

- For example, the first observation has a predicted probability of 0.2041, which indicates a 20.4% chance that the individual is a smoker.
- The second observation has a predicted probability of 0.2055, meaning a slightly higher 20.6% chance of being a smoker, and so on.

These predicted probabilities suggest that, according to the model, the likelihood of being a smoker increases slightly as BMI rises, but the overall probabilities remain below 50% for all observed individuals. The small differences in predicted probabilities indicate a weak relationship between BMI and the likelihood of smoking, as reflected by the small coefficient from the `estimate_beta` function.

#### Explanation:

Here’s how the `predict_prob()` function works:

1. **Adding the Intercept Term:**
   The first step is to ensure that the model accounts for an intercept term. This is done by adding a column of ones to the predictor matrix \(X\) before making any predictions. This column represents the intercept of the logistic regression model.

2. **Applying the Logistic Function (Sigmoid):**
   The function then applies the logistic (sigmoid) function to the linear combination of the predictors and the estimated beta coefficients. The logistic function is defined as:

   \[
   p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 \cdot \text{BMI})}}
   \]

   where:
   - \(p_i\) is the predicted probability of being a smoker for the \(i\)-th observation,
   - \(\beta_0\) is the intercept,
   - \(\beta_1\) is the coefficient for BMI,
   - and \(\text{BMI}\) is the predictor variable (Body Mass Index).

3. **Returning the Predicted Probabilities:**
   After applying the logistic function to each observation in the dataset, the function returns the predicted probabilities. These probabilities represent the likelihood of an individual being a smoker based on their BMI.



### Step 3: Bootstrapping Confidence Intervals for Beta Coefficients

Now that we have estimated the beta coefficients using logistic regression, we can use bootstrapping to estimate confidence intervals for these coefficients. Bootstrapping is a resampling technique that allows us to estimate the sampling distribution of a statistic by resampling with replacement from the original data.

Here’s how to apply the `bootstrap_ci()` function:

```{r bootstrap-ci-example}


# Select the columns we are interested in
#X <- expenses[, c("bmi", "children")]  # Predictors: BMI and Children
#y <- ifelse(expenses$smoker == "yes", 1, 0)  # Response: Smoking status (1 for yes, 0 for no)

# Perform bootstrap to calculate confidence intervals for the beta coefficients
#bootstrap_result <- bootstrap_ci(X, y, num_bootstraps = 100, alpha = 0.05)

# Display the confidence intervals
#bootstrap_result

```

#### Explanation:

Here’s how the `bootstrap_ci()` function works:

1. **Preparing the Data:**
   The first step is to add an intercept term to the predictor matrix \(X\), just like in the `estimate_beta()` function. This ensures that the logistic regression model accounts for an intercept when estimating the coefficients.

2. **Bootstrapping Procedure:**
   The function performs a bootstrap procedure by resampling the data with replacement. For each bootstrap sample:
   - A random subset of the data is drawn (with replacement), meaning some observations may appear multiple times, while others may be omitted.
   - The function then estimates the beta coefficients for the bootstrap sample using the `estimate_beta()` function.

   This resampling process is repeated `num_bootstraps` times (the default is 20). Each time, the `estimate_beta()` function is applied to the bootstrap sample to compute the corresponding beta coefficients.

3. **Storing the Bootstrap Estimates:**
   The estimated beta coefficients from each bootstrap sample are stored in a matrix (`beta_boot`). Each row of this matrix corresponds to a different bootstrap sample, and each column corresponds to a different beta coefficient.

4. **Computing Confidence Intervals:**
   After performing the bootstrap procedure, the function calculates the percentile-based confidence intervals for each beta coefficient. This is done by applying the `quantile()` function across each column of the `beta_boot` matrix:
   - The lower bound of the confidence interval is calculated by taking the `alpha / 2` quantile (default is 0.025, corresponding to a 95% confidence interval).
   - The upper bound of the confidence interval is calculated by taking the `1 - alpha / 2` quantile (default is 0.975, corresponding to a 95% confidence interval).

   The result is a matrix where each row represents a beta coefficient, and the two columns represent the lower and upper bounds of the confidence interval for that coefficient.

5. **Returning the Results:**
   The function returns a matrix with the lower and upper bounds of the confidence intervals for each estimated beta coefficient. This allows you to see the range within which the true beta coefficients are likely to fall with a certain level of confidence (e.g., 95%).

In this case, the `bootstrap_ci()` function helps quantify the uncertainty of the estimated beta coefficients by providing confidence intervals based on the resampled data. This can be particularly useful in evaluating the stability of the coefficients and understanding the precision of your model.


#References 
# https://chatgpt.com/share/6751295b-685c-8010-9fc9-176f596d9850
