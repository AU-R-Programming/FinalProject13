---
title: "Getting Started with FinalProject13"
output: rmarkdown::html_vignette
author: "Braxton Stacey, Joshua Fahlgren, Thomas Jackson"
vignette: >
  %\VignetteIndexEntry{tutorial_name}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(FinalProject13)
```

## Introduction

Welcome to the **FinalProject13** package! This package implements supervised binary classification using numerical optimization techniques, with a particular focus on logistic regression. It is designed to help users efficiently estimate the coefficients of logistic regression models and generate confidence intervals using bootstrapping. Additionally, the package includes tools for model evaluation, including confusion matrix calculations and other diagnostic metrics.

Whether you are exploring relationships in your data or building robust predictive models, **FinalProject13** provides a set of functions for model fitting, evaluation, and interpretation, helping you easily integrate these tools into your analytical workflow.

In this vignette, we will guide you through the primary functionalities of the package, showing you how to:

- Estimate logistic regression coefficients using numerical optimization

- Generate bootstrap confidence intervals for model parameters

- Make predictions and evaluate model performance using various metrics

Let's get started!

## Data Preprocessing

Before we begin using the functions in the **FinalProject13** package, let's first preprocess the data. For this example, we will be using a dataset called **expenses.csv**, which contains various information, including smoking status, BMI, and the number of children for a set of individuals. Our goal is to process this data and prepare it for binary classification analysis.

## Step 1: Loading the Data

We will begin by loading the **expenses.csv** file. This dataset includes multiple columns, but we are particularly interested in three columns:

- The **charges** column, which indicates an individuals medical charges.

- The **age** column, which indicates how old an individual is.

- The **bmi** colum, which represents the Body Mass Index (BMI) of each individual.

For our purposes, we will create new adjusted columns based off of the previously mentioned columns. 

- **charges_above_median** will consist of 1's for if the individuals charges are above the median of the dataset or a 0 if it is less than or equal to the median of the dataset, the median of the dataset being 9382.

- **years_older_than_min** will consist of a numeric value that indicates how many years older an individual is than the minimum age value of the dataset, which is 18.

- **bmi_greater_than_min** will consist of a numeric value that indicates how much greater an individual's bmi is than the minimum bmi value of the dataset, which is 15.96.

```{r}
# Load the dataset from the 'inst/extdata' directory in the package
expenses <- read.csv(system.file("extdata", "expenses.csv", package = "FinalProjectGroup13"))
head(expenses)

# Step 2: Converting Data for Use
# Set the median value for charges
median_charges <- 9382

# Create a new column indicating whether charges are greater than the median
expenses$charges_above_median <- ifelse(expenses$charges > median_charges, 1, 0)

# Create a new column for age in years older than 18
expenses$years_older_than_min <- expenses$age - 18

# Create a new column for bmi greater than min of 15.96
expenses$bmi_greater_than_min <- expenses$bmi - 15.96

# Step 3: Inspecting the Data
# Inspect the 'years_older_than_mi' and 'bmi_greater_than_min' columns for any issues
summary(expenses$years_older_than_min)
summary(expenses$bmi_greater_than_min)

# Step 4: Data Summary
# Summary of the processed data
summary(expenses)
```

## Applying Package Functions to the Data

Now that we have preprocessed the data, let's begin applying the functions from the **FinalProject13** package to perform logistic regression analysis on the data.

## Step 1: Estimating Beta Coefficients Using Logistic Regression

The first step in logistic regression is to estimate the coefficients for the predictor variables. We will use the `estimate_beta()` function from our package to perform this estimation. The function takes a matrix of predictor variables (in our case, the **years_older_than_min** column and the **bmi_greater_than_min** column) and a binary response variable (the **charges_above_median** column), and returns the estimated beta coefficients.
```{r}
# Ensure that both 'bmi' and 'children' columns are numeric
expenses$years_older_than_min <- as.numeric(expenses$years_older_than_min)
expenses$bmi_greater_than_min <- as.numeric(expenses$bmi_greater_than_min)

# Create the predictor matrix (bmi and children columns) as a numeric matrix
X <- as.matrix(expenses[, c("years_older_than_min", "bmi_greater_than_min")])

# Response vector (smoker column)
y <- expenses$charges_above_median

# Apply the estimate_beta function to estimate the beta coefficients
model <- estimate_beta(X, y)

# View the estimated coefficients
model$coefficients

```

### Explaining the Results
Our `estimate_beta()` function returned us three values:

#### 1. The Intercept (-2.073).

This value indicates the log-odds of an individuals charges being above the median when both years_older_than_min and bmi_greater_than_min are equal to 0. In other words, it's the base. It being large and negative indicates that an individual with the base values for the predictor variables is relatively unlikely to have their charges be above the median.

#### 2. The Coefficient for years_older_than_min (0.0874)

This value indicates the change in the log-odds of an individuals charges being above the median for each additional year above 18. The number being positive and relatively large indicates relatively significantly that the older an individual is they are more likely to have their charges be above the median.

#### 3. The Coefficient for years_older_than_min (0.0153)

This value indicates the change in the log-odds of an individuals charges being above the median for each additional additonal unit increase in BMI above 15.96. The number being positive and relatively large indicates relatively significantly (though not as significantly as for years_older_than_min) that the higher bmi an individual has they are more likely to have their charges be above the median.

## Step 2: Bootstrapping Confidence Intervals for Beta Coefficients

Now that we have estimated the beta coefficients using logistic regression, we can use bootstrapping to estimate confidence intervals for these coefficients. Bootstrapping is a resampling technique that allows us to estimate the sampling distribution of a statistic by resampling with replacement from the original data. We can do this using the `bootstrap_ci()` function from our package:

```{r bootstrap-ci-example}

# Perform bootstrap to calculate confidence intervals for the beta coefficients
bootstrap_result <- bootstrap_ci(X, y, alpha = 0.05, num_bootstrap = 20)

# Display the confidence intervals
print(bootstrap_result)

```

### Explaining the Results:
Our `bootstrap_ci()` function yielded us with two vectors consisting of three numbers. The first vector represents the lower bound of the confident interval (in our case 95% given alpha is 0.05) for each of the estimated coefficients, while the second vector represents the upper bound.

#### Intercept
Our intercept confidence interval ranges from -2.3432 to -1.6474, indicating that on both the high and low end an individual with the minimum age and bmi is unlikely ot have charges above the median.

#### years_older_than_min
Our years_older_than_min confidence interval ranges from 0.07998 to 0.09822, indicating that this predictor is very likely to have a relatively strong positive correlation with charges_above_median as thought before.

#### bmi_greater_than_min
Our bmi_greater_than_min confidence interval ranges from -0.01034 to 0.02901, this indicates that our determination that bmi has a positive correlation with charges_above_median may actually be incorrect, although we cannot know for sure either way as the true coefficient could be anywhere within this range, which spans from negative to positive.

## Step 3: Predicting Probabilities for Logistic Regression

After estimating the beta coefficients using the logistic regression model, the next step is to predict the probabilities of the outcome variable (whether an individual has charges above the median) for each observation in the dataset. This is done using the `predict_prob()` function from our package.

The `predict_prob()` function takes the predictor matrix \(X\) and the estimated beta coefficients, both gathered earlier, and it returns the predicted probabilities for each observation in the dataset. These predicted probabilities indicate the likelihood that each individual has charges above the median based on their age and bmi.

The formula used for prediction is the logistic function applied to the linear predictor:

\[
p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 \cdot \text{BMI})}}
\]

where:
- \(p_i\) is the predicted probability of being a smoker for the \(i\)-th observation,
- \(\beta_0\) is the intercept,
- \(\beta_1\) is the coefficient for BMI,
- and \(\text{BMI}\) is the predictor variable.

Using the estimated coefficients, the `predict_prob()` function computes the probability for each observation in the dataset, which can then be used for classification or further model evaluation.


```{r}
# Apply the predict_prob function to generate predicted probabilities
predictions <- predict_prob(X, model$coefficients)

# View the predicted probabilities for the first few observations
head(predictions)

```

### Explaining the Results:
Our `predict_prob()` function yielded us with predicted probabilities for each individual in the dataset. Looking at the first few, we can see that the prediction function is working as expected. For example, the first individual is predicted to be very unlikely to have charges above the median as they have a predicted probability of 14%. This lines up with our coefficients, as, looking at our dataset, the first individual is young with a low bmi. In the other direction, we can look at the highest probability of having charges above the median in the group, which is the fifth individual. This lines up with our coefficients, as, looking at our dataset, the individual has the second highest age of our group of 6 and the third highest bmi.

### Step 4: Creating a Confusion Matrix and Generating Related Metrics

Finally, we can use the coefficients we have generated to generate a confusion matrix with the `confusion_metrics` function in our package, as well as a number of other metrics. This information is crucial to the process as it will help us to determine the quality of our model in making predictions.

- The confusion matrix will indicate how many correct and incorrect predictions our model made.

- The prevalence metric will indicate how often charges are above the median. 

- The accuracy metric will indicate the proportion of predictions the model made that were correct. 

- The sensitivity metric will indicate the proportion of actual positive cases (charges above the median) that were correctly identified by the model.

- The specificity metric will indicate the proportion of actual negative cases (charges below the median) that were correctly identified.

- The false discovery rate will indicate the proportion of predicted positive cases that were actually negative.

- The diagnostic odds ratio metric will indicate the ratio of the odds of a true positive to the odds of a false positive.

```{r}

# Calculate confusion metrics using a cutoff of 0.5 (common threshold for binary classification)
metrics <- confusion_metrics(y, predictions, cutoff = 0.5)

# Print confusion matrix and evaluation metrics
cat("\nConfusion Matrix:\n")
print(metrics$confusion_matrix)

cat("\nModel Performance Metrics:\n")
cat("Prevalence: ", metrics$prevalence, "\n")
cat("Accuracy: ", metrics$accuracy, "\n")
cat("Sensitivity: ", metrics$sensitivity, "\n")
cat("Specificity: ", metrics$specificity, "\n")
cat("False Discovery Rate (FDR): ", metrics$fdr, "\n")
cat("Diagnostic Odds Ratio (DOR): ", metrics$dor, "\n")

```
### Explaining the Results:

#### Confusion Matrix
In our confusion matrix we can see that our model is generating more correct outcomes than incorrect outomes, indicating positive things about our model.

#### Prevalence
Our value here is 0.5. This indicates that half of the observations in your dataset have charges greater than the median, which makes sense considering the nature of our variable.

#### Accuracy
Our value here is 71.23%. This means that the model is correctly classifying around 71% of the cases, which indicates positive things about our model

#### Sensitivity
Our value here is 71.60%. This means the model is identifying about 71.6% of the true positives, which indicates positive things about our model.

#### Specificity
Our value here is 70.85%, meaning the model is correctly identifying around 70.85% of the true negatives, which indicates positive things about our model.

#### False Discovery Rate
Our value here is 28.93%, indicating that only about 29% of the predicted positive cases were actually false positives, which indicates positive things about our model.

#### Diagnostic Odds Ratio
Our value here is 6.13, suggesting that the model has some discriminatory power between the classes (charges above and below the median), which indicates positive things about our model.

#### Summary
All metrics considered, it appears our model is strong and that the coefficients of age and bmi can be used together effectively to predict whether an individual has charges above the median.


## In Closing
Through this process, the merits of the **FinalProject13** package become clear. We were able to estimate coefficients from a dataset using `estimate_beta`, determine the confidence interval for the coefficients using `bootstrap_ci`, create and use a prediction model on the dataset using the coefficients using `predict_prob`, and finally we were able to determine the quality of that model using `confusion_metrics`.


### References 
#### https://chatgpt.com/share/6751295b-685c-8010-9fc9-176f596d9850
#### https://chatgpt.com/share/675277dc-b844-800a-ab65-b7aa75818653
#### https://chatgpt.com/share/675277ad-e7ac-800a-a8d2-0ff547dc9b54
