---
title: "Getting Started with FinalProjectGroup13"
output: rmarkdown::html_vignette
author: "Braxton Stacey, Joshua Fahlgren, Thomas Jackson"
vignette: >
  %\VignetteIndexEntry{tutorial_name}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(FinalProjectGroup13)
```


## Introduction

Welcome to the **FinalProjectGroup13** package! This package implements supervised binary classification using numerical optimization techniques, with a particular focus on logistic regression. It is designed to help users efficiently estimate the coefficients of logistic regression models and generate confidence intervals using bootstrapping. Additionally, the package includes tools for model evaluation, including confusion matrix calculations and other diagnostic metrics.

Whether you are exploring relationships in your data or building robust predictive models, **FinalProjectGroup13** provides a set of functions for model fitting, evaluation, and interpretation, helping you easily integrate these tools into your analytical workflow.

In this vignette, we will guide you through the primary functionalities of the package, showing you how to:
- Estimate logistic regression coefficients using numerical optimization
- Generate bootstrap confidence intervals for model parameters
- Make predictions and evaluate model performance using various metrics

Let's get started!

## Data Preprocessing

Before we begin using the functions in the **FinalProjectGroup13** package, let's first preprocess the data. For this example, we will be using a dataset called **expenses.csv**, which contains various information including the number of children and smoking status for a set of individuals. Our goal is to process this data and prepare it for binary classification analysis.

### Step 1: Loading the Data

We will begin by loading the **expenses.csv** file. This dataset includes multiple columns, but we are particularly interested in two columns:
- The **smoker** column (5th column), which indicates whether an individual smokes or not (with values "yes" or "no").
- The **children** column (4th column), which represents the number of children each individual has.

```{r}
# Load the dataset from the 'inst/extdata' directory in the package
expenses <- read.csv(system.file("extdata", "expenses.csv", package = "FinalProjectGroup13"))
head(expenses)

# Step 2: Converting Categorical Data to Binary

# Convert 'smoker' column to binary (1 for yes, 0 for no)
expenses$smoker <- ifelse(expenses$smoker == "yes", 1, 0)

# Step 3: Inspecting the Data

# Inspect the 'children' column for any issues
summary(expenses$children)

# Step 4: Data Summary

# Summary of the processed data
summary(expenses)
```

## Applying Package Functions to the Data

Now that we have preprocessed the data, let's begin applying the functions from the **FinalProjectGroup13** package to perform logistic regression analysis on the data. We will use the **smoker** and **children** columns for this analysis.

### Step 1: Estimating Beta Coefficients Using Logistic Regression

The first step in logistic regression is to estimate the coefficients for the predictor variables. We will use the `estimate_beta()` function to perform this estimation. The function takes a matrix of predictor variables (in our case, the **children** column) and a binary response variable (the **smoker** column), and returns the estimated beta coefficients.


```{r}
# Create the predictor matrix (children column) and response vector (smoker column)
X <- expenses$children
y <- expenses$smoker

# Apply the estimate_beta function to estimate the beta coefficients
model <- estimate_beta(X, y)

# View the estimated coefficients
model$coefficients
```

#### Explanation:

Here’s how the `estimate_beta()` function works:

1. **Adding the Intercept Term:**
   The first step is to add a column of ones to the predictor matrix \(X\) to account for the intercept term in the logistic regression model. This is necessary because the logistic regression model includes a constant term (\(\beta_0\)) that does not depend on the predictors.

2. **Calculating Starting Beta Coefficients:**
   Next, the function calculates the starting values for the beta coefficients using Ordinary Least Squares (OLS) regression. This is done by solving the normal equation:

   \[
   \beta_{\text{OLS}} = (X^T X)^{-1} X^T y
   \]

   where \(X\) is the predictor matrix (including the intercept term), and \(y\) is the response variable (smoker: yes/no). These initial estimates are used as starting values for the optimization algorithm.

3. **Defining the Loss Function:**
   The loss function used in logistic regression is the negative log-likelihood function. This function measures how well the model fits the data by comparing the predicted probabilities to the actual response values. The logistic (sigmoid) function is applied to the linear combination of the predictors and their corresponding coefficients, and the log-likelihood is computed.

   The formula for the log-likelihood for logistic regression is:

   \[
   \text{Log-Likelihood} = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
   \]

   where \(p_i\) is the predicted probability for the \(i\)-th observation, calculated using the logistic function.

4. **Optimization:**
   The function uses the `optim()` function in R to find the values of the beta coefficients that minimize the negative log-likelihood. The optimization process adjusts the beta coefficients to find the best-fitting model. The `optim()` function returns the optimized beta coefficients that minimize the loss function.

5. **Returning the Results:**
   Once the optimization is complete, the function returns a list containing:
   - The optimized beta coefficients (`coefficients`),
   - The fitted probabilities (`fitted_values`),
   - The starting beta coefficients from the OLS estimation (`starting_beta`), and
   - Information about the optimization process, including the value of the loss function and the number of iterations.

The `estimate_beta()` function provides the estimated coefficients for the logistic regression model, which can then be used for predictions, hypothesis testing, or model evaluation.


### Step 2: Predicting Probabilities for Logistic Regression

After estimating the beta coefficients using the logistic regression model, the next step is to predict the probabilities of the outcome variable (whether an individual is a smoker) for each observation in the dataset. This is done using the `predict_prob()` function from our package.

The `predict_prob()` function takes the predictor matrix \(X\) (which includes the **children** variable) and the estimated beta coefficients, and it returns the predicted probabilities for each observation in the dataset.

```{r}
# Apply the predict_prob function to generate predicted probabilities
predictions <- predict_prob(X, model$coefficients)

# View the predicted probabilities for the first few observations
head(predictions)
```

#### Explanation:

Here’s how the `predict_prob()` function works:

1. **Adding the Intercept Term:**
   The first step is to ensure that the model accounts for an intercept term. This is done by adding a column of ones to the predictor matrix \(X\) before making any predictions. This column represents the intercept of the logistic regression model.

2. **Applying the Logistic Function (Sigmoid):**
   The function then applies the logistic (sigmoid) function to the linear combination of the predictors and the estimated beta coefficients. The logistic function is defined as:

   \[
   p = \frac{1}{1 + e^{-z}}
   \]

   where \(p\) is the predicted probability, and \(z = \beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \ldots\) is the linear combination of the predictors with their corresponding estimated coefficients. The logistic function ensures that the predicted probabilities are between 0 and 1.

3. **Returning the Predicted Probabilities:**
   After applying the logistic function to each observation in the dataset, the function returns the predicted probabilities. These probabilities represent the likelihood of an individual being a smoker based on their number of children (the **children** variable).

In this case, the `predictions` vector will contain the predicted probabilities for each individual in the dataset. These probabilities indicate the likelihood that each individual smokes, given their number of children.

For example, if the predicted probability is close to 1, it indicates a high likelihood of the individual being a smoker, while a predicted probability close to 0 indicates a low likelihood of smoking.

The `head(predictions)` command shows the predicted probabilities for the first few observations in the dataset.


### Step 3: Bootstrapping Confidence Intervals for Beta Coefficients

Now that we have estimated the beta coefficients using logistic regression, we can use bootstrapping to estimate confidence intervals for these coefficients. Bootstrapping is a resampling technique that allows us to estimate the sampling distribution of a statistic by resampling with replacement from the original data.

Here’s how to apply the `bootstrap_ci()` function:

```{r bootstrap-ci-example}
# Load the data
expenses <- read.csv("expenses.csv")

# Select the columns we are interested in
X <- expenses[, c("children")]  # Predictor: Number of children
y <- ifelse(expenses$smoker == "yes", 1, 0)  # Response: Smoking status (1 for yes, 0 for no)

# Perform bootstrap to calculate confidence intervals for the beta coefficients
bootstrap_result <- bootstrap_ci(X, y, num_bootstraps = 100, alpha = 0.05)

# Display the confidence intervals
bootstrap_result
```

#### Explanation:

Here’s how the `bootstrap_ci()` function works:

1. **Preparing the Data:**
   The first step is to add an intercept term to the predictor matrix \(X\), just like in the `estimate_beta()` function. This ensures that the logistic regression model accounts for an intercept when estimating the coefficients.

2. **Bootstrapping Procedure:**
   The function performs a bootstrap procedure by resampling the data with replacement. For each bootstrap sample:
   - A random subset of the data is drawn (with replacement), meaning some observations may appear multiple times, while others may be omitted.
   - The function then estimates the beta coefficients for the bootstrap sample using the `estimate_beta()` function.

   This resampling process is repeated `num_bootstraps` times (the default is 20). Each time, the `estimate_beta()` function is applied to the bootstrap sample to compute the corresponding beta coefficients.

3. **Storing the Bootstrap Estimates:**
   The estimated beta coefficients from each bootstrap sample are stored in a matrix (`beta_boot`). Each row of this matrix corresponds to a different bootstrap sample, and each column corresponds to a different beta coefficient.

4. **Computing Confidence Intervals:**
   After performing the bootstrap procedure, the function calculates the percentile-based confidence intervals for each beta coefficient. This is done by applying the `quantile()` function across each column of the `beta_boot` matrix:
   - The lower bound of the confidence interval is calculated by taking the `alpha / 2` quantile (default is 0.025, corresponding to a 95% confidence interval).
   - The upper bound of the confidence interval is calculated by taking the `1 - alpha / 2` quantile (default is 0.975, corresponding to a 95% confidence interval).

   The result is a matrix where each row represents a beta coefficient, and the two columns represent the lower and upper bounds of the confidence interval for that coefficient.

5. **Returning the Results:**
   The function returns a matrix with the lower and upper bounds of the confidence intervals for each estimated beta coefficient. This allows you to see the range within which the true beta coefficients are likely to fall with a certain level of confidence (e.g., 95%).

In this case, the `bootstrap_ci()` function helps quantify the uncertainty of the estimated beta coefficients by providing confidence intervals based on the resampled data. This can be particularly useful in evaluating the stability of the coefficients and understanding the precision of your model.

